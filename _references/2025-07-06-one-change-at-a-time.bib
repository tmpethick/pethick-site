@article{yang2023spectral,
  title={A spectral condition for feature learning},
  author={Yang, Greg and Simon, James B and Bernstein, Jeremy},
  journal={arXiv preprint arXiv:2310.17813},
  year={2023}
}

@article{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{balles2020geometry,
  title={The geometry of sign gradient descent},
  author={Balles, Lukas and Pedregosa, Fabian and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2002.08056},
  year={2020}
}

@article{yu2017block,
  title={Block-normalized gradient method: An empirical study for training deep neural network},
  author={Yu, Adams Wei and Huang, Lei and Lin, Qihang and Salakhutdinov, Ruslan and Carbonell, Jaime},
  journal={arXiv preprint arXiv:1707.04822},
  year={2017}
}

@article{ginsburg2019stochastic,
  title={Stochastic gradient methods with layer-wise adaptive moments for training of deep networks},
  author={Ginsburg, Boris and Castonguay, Patrice and Hrinchuk, Oleksii and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Nguyen, Huyen and Zhang, Yang and Cohen, Jonathan M},
  journal={arXiv preprint arXiv:1905.11286},
  year={2019}
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{large2024scalable,
  title={Scalable Optimization in the Modular Norm},
  author={Large, Tim and Liu, Yang and Huh, Minyoung and Bahng, Hyojin and Isola, Phillip and Bernstein, Jeremy},
  journal={arXiv preprint arXiv:2405.14813},
  year={2024}
}

@article{an2025asgo,
  title={Asgo: Adaptive structured gradient optimization},
  author={An, Kang and Liu, Yuxing and Pan, Rui and Ren, Yi and Ma, Shiqian and Goldfarb, Donald and Zhang, Tong},
  journal={arXiv preprint arXiv:2503.20762},
  year={2025}
}

@misc{kovalev2025,
      title={SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration}, 
      author={Dmitry Kovalev},
      year={2025},
      eprint={2506.23803},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.23803}, 
}

@article{loshchilov2024ngpt,
  title={ngpt: Normalized transformer with representation learning on the hypersphere},
  author={Loshchilov, Ilya and Hsieh, Cheng-Ping and Sun, Simeng and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2410.01131},
  year={2024}
}

@article{defazio2020momentum,
  title={Momentum via primal averaging: theoretical insights and learning rate schedules for non-convex optimization},
  author={Defazio, Aaron},
  journal={arXiv preprint arXiv:2010.00406},
  year={2020}
}

@article{tao2018primal,
  title={Primal averaging: A new gradient evaluation step to attain the optimal individual convergence},
  author={Tao, Wei and Pan, Zhisong and Wu, Gaowei and Tao, Qing},
  journal={IEEE transactions on cybernetics},
  volume={50},
  number={2},
  pages={835--845},
  year={2018},
  publisher={IEEE}
}

@article{pethick2025training,
  title={Training Deep Learning Models with Norm-Constrained LMOs},
  author={Pethick, Thomas and Xie, Wanyun and Antonakopoulos, Kimon and Zhu, Zhenyu and Silveti-Falls, Antonio and Cevher, Volkan},
  journal={arXiv preprint arXiv:2502.07529},
  year={2025}
}
