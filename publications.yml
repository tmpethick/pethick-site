- title: "Solving stochastic weak Minty variational inequalities without increasing batch size"
  authors: Thomas Pethick, Olivier Fercoq, Puya Latafat, Panagiotis Patrinos and Volkan Cevher
  conference: International Conference on Learning Representations (ICLR) 2023
  paper: https://openreview.net/pdf?id=ejR4E1jaH9k
  code: https://github.com/LIONS-EPFL/stochastic-weak-minty-code
  abstract: This paper introduces a family of stochastic extragradient-type algorithms for a class of nonconvex-nonconcave problems characterized by the weak Minty variational inequality (MVI). Unlike existing results on extragradient methods in the monotone setting, employing diminishing stepsizes is no longer possible in the weak MVI setting. This has led to approaches such as increasing batch sizes per iteration which can however be prohibitively expensive. In contrast, our proposed methods involves two stepsizes and only requires one additional oracle evaluation per iteration. We show that it is possible to keep one fixed stepsize while it is only the second stepsize that is taken to be diminishing, making it interesting even in the monotone setting. Almost sure convergence is established and we provide a unified analysis for this family of schemes which contains a nonlinear generalization of the celebrated primal dual hybrid gradient algorithm.
- title: "Adversarial Training descends without descent: Finding actual descent directions based on Danskin's theorem"
  authors: Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Pethick and Volkan Cevher
  conference: International Conference on Learning Representations (ICLR) 2023
  paper: https://openreview.net/pdf?id=I3HCE7Ro78H
  code: 
  abstract: Adversarial Training using a strong first-order adversary (PGD) is the gold standard for training Deep Neural Networks that are robust to adversarial examples. We show that, contrary to the general understanding of the method, the gradient at an optimal adversarial example may increase, rather than decrease, the adversarially robust loss. This holds independently of the learning rate. More precisely, we provide a counterexample to a corollary of Danskin's Theorem presented in the seminal paper of Madry et al. (2018) which states that a solution of the inner maximization problem can yield a descent direction for the adversarially robust loss. Based on a correct interpretation of Danskin's Theorem, we propose Danskin's Descent Direction (DDD) and we verify experimentally that it provides better directions than those obtained by a PGD adversary. Using the CIFAR10 dataset we further provide a real world example showing that our method achieves a steeper increase in robustness levels in the early stages of training, and is more stable than the PGD baseline.
- title: "Revisiting adversarial training for the worst-performing class"
  authors: Thomas Pethick and Grigorios Chrysos and Volkan Cevher
  conference: Transactions on Machine Learning Research (TMLR) 2022
  paper: https://openreview.net/pdf?id=wkecshlYxI
  code: https://github.com/LIONS-EPFL/class-focused-online-learning-code
  abstract: Despite progress in adversarial training (AT), there is a substantial gap between the top-performing and worst-performing classes in many datasets. For example, on CIFAR10, the accuracies for the best and worst classes are 74% and 23%, respectively. We argue that this gap can be reduced by explicitly optimizing for the worst-performing class, resulting in a min-max-max optimization formulation.  Our method, called class focused online learning (CFOL), includes high probability convergence guarantees for the worst class loss and can be easily integrated into existing training setups with minimal computational overhead.  We demonstrate an improvement to 32% in the worst class accuracy on CIFAR10, and we observe consistent behavior across CIFAR100 and STL10.  Our study highlights the importance of moving beyond average accuracy,  which is particularly important in safety-critical applications.
- title: "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems"
  authors: Thomas Pethick, Puya Latafat, Panagiotis Patrinos, Olivier Fercoq, Volkan Cevher
  conference: The International Conference on Learning Representations (ICLR) 2022
  paper: https://infoscience.epfl.ch/record/291889/files/escaping_limit_cycles_global_c.pdf
  code: https://github.com/LIONS-EPFL/weak-minty-code/
  abstract: This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is computationally intractable. This observation has recently motivated the study of structures sufficient for convergence of first order methods in the more general setting of variational inequalities when the so-called weak Minty variational inequality (MVI) holds. This problem class captures non-trivial structures as we demonstrate with examples, for which a large family of existing algorithms provably converge to limit cycles. Our results require a less restrictive parameter range in the weak MVI compared to what is previously known, thus extending the applicability of our scheme. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Our scheme also converges globally even in settings where the underlying operator exhibits limit cycles. Moreover, a variant with stochastic oracles is proposed---making it directly relevant for training of generative adversarial networks. For the stochastic algorithm only one of the stepsizes is required to be diminishing while the other may remain constant, making it interesting even in the monotone setting.
- title: "Sifting through the Noise: Universal First-Order Methods for Stochastic Variational Inequalities"
  authors:  Kimon Antonakopoulos, Thomas Pethick, Ali Kavis, Panayotis Mertikopoulos, Volkan Cevher
  conference: Neural Information Processing Systems (NeurIPS) 2021
  paper: https://infoscience.epfl.ch/record/289992/files/sifting_through_the_noise_univ-Supplementary%20Material.pdf
  code: https://github.com/LIONS-EPFL/vi-relative-noise-numerics
  abstract: "We examine a flexible algorithmic framework for solving monotone variational inequalities in the presence of randomness and uncertainty. The proposed template encompasses a wide range of popular first-order methods, including dual averaging, dual extrapolation and optimistic gradient algorithms - both adaptive and non-adaptive. Our first result is that the algorithm achieves the optimal rates of convergence for cocoercive problems when the profile of the randomness is known to the optimizer: O (1/√T) for absolute noise profiles, and O (1/T) for relative ones. Subsequently, we drop all prior knowledge requirements (the absolute/ relative variance of the randomness affecting the problem, the operator's cocoercivity constant, etc.), and we analyze an adaptive instance of the method that gracefully interpolates between the above rates – i.e., it achieves O (1/√T) and O (1/T) in the absolute and relative cases, respectively. To our knowledge, this is the first universality result of its kind in the literature and, somewhat surprisingly, it shows that an extra-gradient proxy step is not required to achieve optimal rates."
- title: "Subquadratic Overparameterization for Shallow Neural Networks"
  authors: Chaehwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, Volkan Cevher
  conference: Neural Information Processing Systems (NeurIPS) 2021
  paper: https://infoscience.epfl.ch/record/289650/files/OverpararXiv%281%29.pdf
  code: https://github.com/LIONS-EPFL/Subquadratic-Overparameterization
  abstract: Overparameterization refers to the important phenomenon where the width of a neural network is chosen such that learning algorithms can provably attain zero loss in nonconvex training. The existing theory establishes such global convergence using various initialization strategies, training modifications, and width scalings. In particular, the state-of-the-art results require the width to scale quadratically with the number of training data under standard initialization strategies used in practice for best generalization performance. In contrast, the most recent results obtain linear scaling either with requiring initializations that lead to the “lazy-training”, or training only a single layer. In this work, we provide an analytical framework that allows us to adopt standard initialization strategies, possibly avoid lazy training, and train all layers simultaneously in basic shallow neural networks while attaining a desirable subquadratic scaling on the network width. We achieve the desiderata via Polyak-Łojasiewicz condition, smoothness, and standard assumptions on data, and use tools from random matrix theory.
